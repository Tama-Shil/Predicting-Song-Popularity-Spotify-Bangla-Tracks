# -*- coding: utf-8 -*-
"""AlgoOnCategorizedData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ldXuYdT1Uyqq00GGYo5CWhAR3kLXXwfg

**importing necessary libraries**
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report
import seaborn as sns

"""**loading dataset & defining features (X) and target (Y)**"""

file_path = '/content/SBT_categorized_dataset.csv'
df = pd.read_csv(file_path)
features = ['danceability', 'energy', 'key', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration', 'time_signature']
X = df[features]
y = df['popularity']
X = pd.get_dummies(X)

"""**splitting data into training and testing sets**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**applying logistic regression**"""

model = LogisticRegression(random_state = 43)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""**applying random forest classifier**"""

model = RandomForestClassifier(random_state = 43)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""**applying KNN**"""

model = KNeighborsClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""**applying SVC**"""

model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""**applying naive bayes**"""

model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""**applying decision tree classifier**"""

model = DecisionTreeClassifier(random_state = 43)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""**applying XGboost classifier**"""

le = LabelEncoder()
y = le.fit_transform(df['popularity'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = XGBClassifier(random_state=43, n_estimators=100, learning_rate=0.1, max_depth=6)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""**comparing algos**"""

results = {
    'Logistic Regression': {'accuracy': 0.52, 'f1_score': 0.373, 'precision': 0.386, 'recall': 0.52},
    'Random Forest': {'accuracy': 0.474, 'f1_score': 0.4118, 'precision': 0.396, 'recall': 0.474},
    'K-Nearest Neighbors': {'accuracy': 0.48, 'f1_score': 0.447, 'precision': 0.432, 'recall': 0.482},
    'Support Vector Machine': {'accuracy': 0.529, 'f1_score': 0.377, 'precision': 0.488, 'recall': 0.529},
    'Naive Bayes': {'accuracy': 0.0468, 'f1_score': 0.0428, 'precision': 0.550, 'recall': 0.0468},
    'Decision Tree': {'accuracy': 0.48, 'f1_score': 0.425, 'precision': 0.4037, 'recall': 0.48},
    'XGBoost': {'accuracy': 0.5028, 'f1_score': 0.416, 'precision': 0.4025, 'recall': 0.5028},
}

df_results = pd.DataFrame(results).T
df_long = df_results.reset_index().melt(id_vars='index', value_vars=['accuracy', 'f1_score', 'precision', 'recall'], var_name='metric', value_name='score')
df_long.rename(columns={'index': 'algorithm'}, inplace=True)
sns.set_palette('viridis')
plt.figure(figsize=(12, 6))
sns.barplot(x='algorithm', y='score', hue='metric', data=df_long)
plt.title('Comparison of Different Measures for Each Algorithm')
plt.xlabel('Algorithm')
plt.ylabel('Score')
plt.xticks(rotation=45, ha='right')
plt.show()